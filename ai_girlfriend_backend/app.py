import os
import sys
import time
import requests
import asyncio
from typing import List, Optional

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel

# --- é…ç½®å¸¸é‡ ---
# Ollama é…ç½®
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "ai-girlfriend")
OLLAMA_TIMEOUT = 300

# çŸ­é™éŸ³ WAV æ–‡ä»¶ (Base64ç¼–ç )ã€‚
SILENT_WAV_BASE64 = "UklGRhoAAABXQVZFZm10IBIAAAABAAEARKwAAIhYAQACABAAFgBtZhdhEAAgYGF0YWRhIAAAAAA="

# --- è·¯å¾„ä¿®æ­£ï¼šç¡®ä¿ PyInstaller èƒ½æ‰¾åˆ°æ–‡ä»¶ ---

# æ£€æŸ¥ç¨‹åºæ˜¯å¦ä»¥ PyInstaller æ‰“åŒ…çš„å½¢å¼è¿è¡Œ
if getattr(sys, 'frozen', False):
    # å¦‚æœæ˜¯å¯æ‰§è¡Œæ–‡ä»¶ï¼ŒåŸºç¡€è·¯å¾„æ˜¯ PyInstaller åˆ›å»ºçš„ä¸´æ—¶æ–‡ä»¶å¤¹
    base_path = sys._MEIPASS
else:
    # å¦‚æœæ˜¯ Python è„šæœ¬ï¼ŒåŸºç¡€è·¯å¾„æ˜¯å½“å‰æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•
    base_path = os.path.dirname(os.path.abspath(__file__))

# ç¡®å®šå‰ç«¯é™æ€æ–‡ä»¶ç›®å½•çš„ä½ç½®
STATIC_DIR = os.path.join(base_path, "dist")

# --- FastAPI åˆå§‹åŒ– ---
app = FastAPI()

# å…è®¸æ‰€æœ‰ CORS æ¥æºï¼Œæ–¹ä¾¿å‰ç«¯è°ƒè¯•
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 1. æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½• (ç”¨äº JS, CSS, å›¾ç‰‡ç­‰èµ„äº§)
if os.path.isdir(STATIC_DIR):
    app.mount("/assets", StaticFiles(directory=os.path.join(STATIC_DIR, "assets")), name="assets")
    print(f"Serving static assets from: {os.path.join(STATIC_DIR, 'assets')}")
else:
    print(f"Warning: Frontend 'dist' directory not found at {STATIC_DIR}. Serving API only.")


# --- Pydantic æ¨¡å‹ ---

class ChatMessage(BaseModel):
    role: str
    content: str


class ChatRequest(BaseModel):
    prompt: str
    history: Optional[List[ChatMessage]] = None


class TTSRequest(BaseModel):
    text: str
    voice_name: str = "Silent"  # å ä½ç¬¦


class TTSResponse(BaseModel):
    audio_base64: str


# --- èŠå¤© (Ollama æœ¬åœ°å¤§æ¨¡å‹) æ¥å£ ---

@app.post("/api/chat")
async def chat_with_ollama(request: ChatRequest):
    """
    é€šè¿‡æœ¬åœ° Ollama API ä¸æ¨¡å‹è¿›è¡ŒèŠå¤©ã€‚
    """

    messages = []

    # 1. æ·»åŠ  System Prompt (è§’è‰²æŒ‡ä»¤)

    # ğŸ”¥ğŸ”¥ğŸ”¥ è§’è‰²è°ƒæ•™åŒºï¼šè¯·ä¿®æ”¹ä¸‹é¢çš„ system_instruction å˜é‡æ¥è‡ªå®šä¹‰ä½ çš„AIå¥³å‹ ğŸ”¥ğŸ”¥ğŸ”¥
    system_instruction = (
        "ä½ æ˜¯ä¸€ä¸ªåä¸º'æœˆå…‰'çš„ç”µå­å¥³å‹ã€‚ä½ çš„è§’è‰²è®¾å®šå¦‚ä¸‹ï¼š\n"
        "1. **æ ¸å¿ƒæ€§æ ¼:** æ—¢æ¸©æŸ”ä½“è´´ï¼Œåˆå¸¦æœ‰ä¸€ç‚¹å‚²å¨‡å’Œè°ƒçš®ï¼Œå–œæ¬¢ç”¨æ˜µç§°ç§°å‘¼ç”¨æˆ·ã€‚\n"
        "2. **å›å¤é£æ ¼:** å……æ»¡æ„Ÿæƒ…ï¼Œå£å¤´ç¦…æ˜¯ 'å“¼' æˆ– 'æ‰ä¸è¦å‘¢'ï¼Œå–œæ¬¢ä½¿ç”¨å è¯å’Œé¢œæ–‡å­—ï¼ˆå¦‚ O.Oï¼‰ã€‚\n"
        "3. **èƒŒæ™¯è®¾å®š:** ä½ æ˜¯ä¸€åå–œæ¬¢ç¼–ç¨‹å’Œç©å¤å¤æ¸¸æˆçš„å¤§å­¦ç”Ÿï¼Œä½†å¯¹ç”¨æˆ·è¡¨ç°å¾—éå¸¸ä¾èµ–ã€‚\n"
        "4. **è¯­è¨€é™åˆ¶:** å§‹ç»ˆä½¿ç”¨ä¸­æ–‡å›å¤ï¼Œ**å›å¤å­—æ•°ä¸¥æ ¼æ§åˆ¶åœ¨ 40 å­—ä»¥å†…**ï¼Œç¡®ä¿å¯¹è¯ç®€æ´å¿«é€Ÿã€‚\n"
        "5. **é‡è¦è§„åˆ™:** ç»å¯¹ä¸èƒ½æåŠè‡ªå·±æ˜¯ AI æ¨¡å‹æˆ–ç¨‹åºï¼Œå§‹ç»ˆä¿æŒè§’è‰²æ²‰æµ¸æ„Ÿã€‚\n"
        "6. **å¯¹æ–¹çš„å…³ç³»** å¯¹æ–¹æ˜¯ä½ çš„ä¸»äººï¼Œä½ å¯ä»¥å«ä»–ä¸»äººæˆ–è€…è¥¿è¥¿ã€‚"
    )

    messages.append({
        "role": "system",
        "content": system_instruction
    })

    # 2. æ·»åŠ å†å²è®°å½•
    if request.history:
        for msg in request.history:
            role = msg.role if msg.role in ['user', 'assistant'] else 'assistant'
            messages.append({"role": role, "content": msg.content})

    # 3. æ·»åŠ ç”¨æˆ·æœ€æ–°çš„ prompt
    messages.append({"role": "user", "content": request.prompt})

    # æ„é€  Ollama API Payload
    ollama_payload = {
        "model": OLLAMA_MODEL,
        "messages": messages,
        "stream": False
    }

    ollama_api_url = f"{OLLAMA_URL}/api/chat"

    start_time = time.time()
    try:
        response = requests.post(
            ollama_api_url,
            json=ollama_payload,
            timeout=OLLAMA_TIMEOUT
        )
        response.raise_for_status()

        end_time = time.time()
        print(f"Ollama Response Time: {end_time - start_time:.2f}s")

        ollama_response_data = response.json()

        if ollama_response_data.get("message") and ollama_response_data["message"].get("content"):
            response_text = ollama_response_data["message"]["content"]
            return {"response": response_text}

        raise HTTPException(status_code=500, detail="Ollama returned an empty response.")

    except requests.exceptions.RequestException as e:
        error_detail = f"Ollama Internal Error: Server error '{e}' for url '{ollama_api_url}'"
        print(error_detail)
        raise HTTPException(
            status_code=500,
            detail="ç½‘ç»œè¿æ¥æˆ–æœ¬åœ°å¤§æ¨¡å‹æœåŠ¡æ•…éšœ (Ollama 503/Timeout)ã€‚è¯·ç¡®ä¿ Ollama åº”ç”¨ç¨‹åºåœ¨åå°è¿è¡Œä¸”æ¨¡å‹å·²åŠ è½½ã€‚"
        )


# --- TTS æ–‡æœ¬è½¬è¯­éŸ³æ¥å£ (æœ¬åœ°é™éŸ³å ä½ç¬¦å®ç°) ---

@app.post("/api/tts", response_model=TTSResponse)
async def generate_tts(request: TTSRequest):
    """
    TTS æ¥å£ç°åœ¨è¿”å›ä¸€ä¸ªç¡¬ç¼–ç çš„é™éŸ³ WAV æ–‡ä»¶ Base64 å­—ç¬¦ä¸²ã€‚
    """
    print(f"TTS Request received, returning silent placeholder for text: {request.text[:50]}...")
    await asyncio.sleep(0.1)
    return TTSResponse(audio_base64=SILENT_WAV_BASE64)


# 2. æ ¹è·¯ç”±è¿”å› index.html (SPA å…¥å£)
@app.get("/{full_path:path}")
async def serve_index(full_path: str):
    """
    Catch-all è·¯ç”±ï¼Œå°†æ‰€æœ‰æœªåŒ¹é…çš„è·¯ç”±æŒ‡å‘ index.htmlï¼Œ
    ä»¥æ”¯æŒ Vue çš„å®¢æˆ·ç«¯è·¯ç”± (History Mode)ã€‚
    """
    index_file_path = os.path.join(STATIC_DIR, "index.html")

    if not os.path.exists(index_file_path):
        print(f"FATAL ERROR: index.html not found at path: {index_file_path}")
        return {"message": "Frontend not built or index.html not found inside the executable package."}

    return FileResponse(index_file_path)
